{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# キーフレーズ抽出アルゴリズムの比較\n",
    "評判のいいSGRank, TextRank, SCAKE, RAKEを比べてみて、どれが配信の特徴を表現できているか確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m spacy download ja_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "\n",
    "ja = textacy.load_spacy_lang(\"ja_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "texts = []\n",
    "\n",
    "datafolder = \"../../../Data/Transcription_raw/\"\n",
    "def get_text(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    text = \"。 \".join(df[\"text\"].tolist())\n",
    "    return text\n",
    "\n",
    "example_files = [datafolder+\"15-4.csv\", datafolder+\"2718-4.csv\", datafolder+\"983-4.csv\", datafolder+\"2084-4.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://qiita.com/python_walker/items/fbc8c3205d01129e6afc\n",
    "def extract_phrases_spacy(text, method):\n",
    "    doc = textacy.make_spacy_doc(text, lang=ja)\n",
    "    keywords_with_score = [\n",
    "        (kps, score) for kps, score in method(doc, normalize=\"lemma\", topn=5)\n",
    "    ]\n",
    "\n",
    "    keywords = [keywords_with_score[i][0] for i in range(len(keywords_with_score))]\n",
    "    scores = [keywords_with_score[i][1] for i in range(len(keywords_with_score))]\n",
    "\n",
    "    return scores, keywords\n",
    "\n",
    "from katodb.Analyzing.rake_ja import Tokenizer, JapaneseRake\n",
    "def extract_phrases_Rake(text):\n",
    "    tokenizer = Tokenizer(rawargs='-r \"C:/Program Files/MeCab/etc/mecabrc\" -u \"C:/Program Files/MeCab/dic/unidic_kato.dic\"')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    rake = JapaneseRake()\n",
    "    \n",
    "    rake.extract_keywords_from_text(tokens)\n",
    "    keywords = rake.get_ranked_phrases_with_scores()\n",
    "    \n",
    "    if len(keywords) > 0:\n",
    "        return [x[0] for x in keywords], [x[1] for x in keywords]\n",
    "    else:\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.extract.keyterms import sgrank, textrank, scake\n",
    "def compare(filename):\n",
    "    text = get_text(filename)\n",
    "    \n",
    "    print(\"sgrank\")\n",
    "    print(extract_phrases_spacy(text, sgrank))\n",
    "    \n",
    "    print(\"textrank\")\n",
    "    print(extract_phrases_spacy(text, textrank))\n",
    "    \n",
    "    print(\"scake\")\n",
    "    print(extract_phrases_spacy(text, scake))\n",
    "    \n",
    "    print(\"rake\")\n",
    "    print(extract_phrases_Rake(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream"
    },
    {
     "ename": "Exception",
     "evalue": "Tokenization error: Input is too long, it can't be more than 49149 bytes, was 296077",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(file)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(get_text(file))\n\u001b[1;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(compare(file))\n",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m, in \u001b[0;36mcompare\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      3\u001b[0m text \u001b[39m=\u001b[39m get_text(filename)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msgrank\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[39mprint\u001b[39m(extract_phrases_spacy(text, sgrank))\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtextrank\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(extract_phrases_spacy(text, textrank))\n",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m, in \u001b[0;36mextract_phrases_spacy\u001b[1;34m(text, method)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_phrases_spacy\u001b[39m(text, method):\n\u001b[1;32m----> 3\u001b[0m     doc \u001b[39m=\u001b[39m textacy\u001b[39m.\u001b[39;49mmake_spacy_doc(text, lang\u001b[39m=\u001b[39;49mja)\n\u001b[0;32m      4\u001b[0m     keywords_with_score \u001b[39m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m         (kps, score) \u001b[39mfor\u001b[39;00m kps, score \u001b[39min\u001b[39;00m method(doc, normalize\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlemma\u001b[39m\u001b[39m\"\u001b[39m, topn\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m      6\u001b[0m     ]\n\u001b[0;32m      8\u001b[0m     keywords \u001b[39m=\u001b[39m [keywords_with_score[i][\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(keywords_with_score))]\n",
      "File \u001b[1;32mc:\\Users\\Hiyoko\\.conda\\envs\\kato-db\\lib\\site-packages\\textacy\\spacier\\core.py:147\u001b[0m, in \u001b[0;36mmake_spacy_doc\u001b[1;34m(data, lang, chunk_size)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39mMake a :class:`spacy.tokens.Doc` from valid inputs, and automatically\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39mload/validate :class:`spacy.language.Language` pipelines to process ``data``.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39m    ValueError\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 147\u001b[0m     \u001b[39mreturn\u001b[39;00m _make_spacy_doc_from_text(data, lang, chunk_size)\n\u001b[0;32m    148\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Doc):\n\u001b[0;32m    149\u001b[0m     \u001b[39mreturn\u001b[39;00m _make_spacy_doc_from_doc(data, lang)\n",
      "File \u001b[1;32mc:\\Users\\Hiyoko\\.conda\\envs\\kato-db\\lib\\site-packages\\textacy\\spacier\\core.py:163\u001b[0m, in \u001b[0;36m_make_spacy_doc_from_text\u001b[1;34m(text, lang, chunk_size)\u001b[0m\n\u001b[0;32m    161\u001b[0m     doc \u001b[39m=\u001b[39m _make_spacy_doc_from_text_chunks(text, spacy_lang, chunk_size)\n\u001b[0;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     doc \u001b[39m=\u001b[39m spacy_lang(text)\n\u001b[0;32m    164\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32mc:\\Users\\Hiyoko\\.conda\\envs\\kato-db\\lib\\site-packages\\spacy\\language.py:1030\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[0;32m   1010\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1011\u001b[0m     text: Union[\u001b[39mstr\u001b[39m, Doc],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     component_cfg: Optional[Dict[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Any]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1015\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Doc:\n\u001b[0;32m   1016\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[39m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[39m    is preserved.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[39m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1030\u001b[0m     doc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_doc(text)\n\u001b[0;32m   1031\u001b[0m     \u001b[39mif\u001b[39;00m component_cfg \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1032\u001b[0m         component_cfg \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Hiyoko\\.conda\\envs\\kato-db\\lib\\site-packages\\spacy\\language.py:1121\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[39mreturn\u001b[39;00m doc_like\n\u001b[0;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(doc_like, \u001b[39mstr\u001b[39m):\n\u001b[1;32m-> 1121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_doc(doc_like)\n\u001b[0;32m   1122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(doc_like, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1123\u001b[0m     \u001b[39mreturn\u001b[39;00m Doc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\u001b[39m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[1;32mc:\\Users\\Hiyoko\\.conda\\envs\\kato-db\\lib\\site-packages\\spacy\\language.py:1113\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length:\n\u001b[0;32m   1110\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1111\u001b[0m         Errors\u001b[39m.\u001b[39mE088\u001b[39m.\u001b[39mformat(length\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(text), max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length)\n\u001b[0;32m   1112\u001b[0m     )\n\u001b[1;32m-> 1113\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(text)\n",
      "File \u001b[1;32mc:\\Users\\Hiyoko\\.conda\\envs\\kato-db\\lib\\site-packages\\spacy\\lang\\ja\\__init__.py:56\u001b[0m, in \u001b[0;36mJapaneseTokenizer.__call__\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Doc:\n\u001b[0;32m     55\u001b[0m     \u001b[39m# convert sudachipy.morpheme.Morpheme to DetailedToken and merge continuous spaces\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     sudachipy_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n\u001b[0;32m     57\u001b[0m     dtokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_dtokens(sudachipy_tokens)\n\u001b[0;32m     58\u001b[0m     dtokens, spaces \u001b[39m=\u001b[39m get_dtokens_and_spaces(dtokens, text)\n",
      "\u001b[1;31mException\u001b[0m: Tokenization error: Input is too long, it can't be more than 49149 bytes, was 296077"
     ]
    }
   ],
   "source": [
    "for file in example_files:\n",
    "    print(file)\n",
    "    print(get_text(file))\n",
    "    print(compare(file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kato-db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
