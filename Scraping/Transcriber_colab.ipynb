{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PHRwLjJmhcu1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "550dd335-6273-43d8-80f3-e0467a28d9d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/guillaumekln/faster-whisper.git\n",
            "  Cloning https://github.com/guillaumekln/faster-whisper.git to /tmp/pip-req-build-lm78vs6y\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/guillaumekln/faster-whisper.git /tmp/pip-req-build-lm78vs6y\n",
            "  Resolved https://github.com/guillaumekln/faster-whisper.git to commit 5c17de17713f65929c7c33add3a9735ff75a945c\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting av==10.* (from faster-whisper==0.7.1)\n",
            "  Using cached av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "Collecting ctranslate2<4,>=3.17 (from faster-whisper==0.7.1)\n",
            "  Using cached ctranslate2-3.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
            "Collecting huggingface_hub>=0.13 (from faster-whisper==0.7.1)\n",
            "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "Collecting tokenizers==0.13.* (from faster-whisper==0.7.1)\n",
            "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Collecting onnxruntime<2,>=1.14 (from faster-whisper==0.7.1)\n",
            "  Using cached onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.17->faster-whisper==0.7.1) (1.22.4)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.17->faster-whisper==0.7.1) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13->faster-whisper==0.7.1) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13->faster-whisper==0.7.1) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13->faster-whisper==0.7.1) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13->faster-whisper==0.7.1) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13->faster-whisper==0.7.1) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13->faster-whisper==0.7.1) (23.1)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper==0.7.1)\n",
            "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==0.7.1) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==0.7.1) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==0.7.1) (1.11.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper==0.7.1)\n",
            "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.13->faster-whisper==0.7.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.13->faster-whisper==0.7.1) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.13->faster-whisper==0.7.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.13->faster-whisper==0.7.1) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper==0.7.1) (1.3.0)\n",
            "Building wheels for collected packages: faster-whisper\n",
            "  Building wheel for faster-whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for faster-whisper: filename=faster_whisper-0.7.1-py3-none-any.whl size=1538687 sha256=8db8c212b30ee400ad11126b2b1b268f7f9b0c2cd355d444807fa88adb870d56\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v6i2raca/wheels/13/44/30/feaa66bd9e2a7a0c04b7cc855ac0d70d5d22106eca624af503\n",
            "Successfully built faster-whisper\n",
            "Installing collected packages: tokenizers, av, humanfriendly, ctranslate2, huggingface_hub, coloredlogs, onnxruntime, faster-whisper\n",
            "Successfully installed av-10.0.0 coloredlogs-15.0.1 ctranslate2-3.17.1 faster-whisper-0.7.1 huggingface_hub-0.16.4 humanfriendly-10.0 onnxruntime-1.15.1 tokenizers-0.13.3\n",
            "Requirement already satisfied: pytube in /usr/local/lib/python3.10/dist-packages (15.0.0)\n"
          ]
        }
      ],
      "source": [
        "#!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install git+https://github.com/guillaumekln/faster-whisper.git\n",
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import whisper\n",
        "from faster_whisper import WhisperModel\n",
        "from pytube import YouTube, Channel\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "import concurrent.futures as cf\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "qfjOCQmjpmnD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "db_dir = \"/content/drive/MyDrive/kato-db/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcs2kK60MPAJ",
        "outputId": "9a8cea61-0fca-4992-a55c-a7752d0bf301"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "video_links.csvから書き起こしを行うクラス\n",
        "'''\n",
        "class Transcriber:\n",
        "  model_num_to_name = {\n",
        "      -1: \"none\",\n",
        "      0: \"tiny\",\n",
        "      1: \"base\",\n",
        "      2: \"small\",\n",
        "      3: \"medium\",\n",
        "      4: \"large-v2\"\n",
        "  }\n",
        "\n",
        "  def __init__(self, model = 4):\n",
        "    self.df_videos = pd.read_csv(db_dir + \"video_links.csv\")\n",
        "    self.model = 3   #使用するwhisperモデル\n",
        "    #self.whisper_model = whisper.load_model(Transcriber.model_num_to_name[model])\n",
        "    self.whisper_model = WhisperModel('zh-plus/faster-whisper-large-v2-japanese-5k-steps', device=\"cuda\", compute_type=\"float16\")\n",
        "\n",
        "  '''\n",
        "  作動する。途中中断しても進捗は保存される。\n",
        "  mode\n",
        "  0->未書き起こしを対象に\n",
        "  1->改善対象（モデルが下位のもの）を対象に\n",
        "  '''\n",
        "  def run(self, mode = 0):\n",
        "    if mode == 0:\n",
        "      index_target = self.get_untranscribeds()\n",
        "    elif mode == 1:\n",
        "      index_target = self.get_improvables()\n",
        "\n",
        "    # with cf.ThreadPoolExecutor(max_workers=2) as executor:\n",
        "    #   futures = []\n",
        "    #   for index in index_target:\n",
        "    #     row = self.df_videos.iloc[index]\n",
        "    #     futures.append(executor.submit(self.transcribe, index, row[\"link\"], self.model))\n",
        "\n",
        "    #   for future in cf.as_completed(futures):\n",
        "    #     index = future.result()\n",
        "    for index in tqdm(index_target):\n",
        "        row = self.df_videos.iloc[index]\n",
        "        self.transcribe(index, row[\"link\"], self.model)\n",
        "        self.df_videos.at[index, \"transcribed\"] = self.model\n",
        "        self.df_videos.to_csv(db_dir + \"video_links.csv\")\n",
        "\n",
        "        print(\"saved:\", index)\n",
        "\n",
        "  '''\n",
        "  未書き起こしの動画のインデックス一覧を取得。一覧はシャッフルされている\n",
        "  '''\n",
        "  def get_untranscribeds(self):\n",
        "    index_untranscribed = list(self.df_videos[self.df_videos[\"transcribed\"] == -1].index)\n",
        "    shuffle(index_untranscribed)\n",
        "    return index_untranscribed\n",
        "\n",
        "  '''\n",
        "  self.modelの方が上位のデータのインデックス一覧を取得。一覧はシャッフルされる。\n",
        "  '''\n",
        "  def get_improvables(self):\n",
        "    index_improvable = list(self.df_videos[self.df_videos[\"transcribed\"] < self.model].index)\n",
        "    shuffle(index_improvable)\n",
        "    return index_improvable\n",
        "\n",
        "  '''\n",
        "  書き起こしを行う。書き起こしファイルの保存、video_links.csvの更新も行われる\n",
        "  '''\n",
        "  def transcribe(self, index, link, model):\n",
        "    #print(\"start:\", index)\n",
        "\n",
        "    #動画をダウンロード\n",
        "    audio_file_name = str(index) + \".mp4\"\n",
        "    audio_file = YouTube(link).streams.filter(only_audio=True).first().download(filename=audio_file_name)\n",
        "\n",
        "    #書き起こし\n",
        "    #transcription = self.whisper_model.transcribe(audio_file, language = \"ja\")[\"segments\"]\n",
        "    segments, _ = self.whisper_model.transcribe(audio_file, language=\"ja\")\n",
        "\n",
        "    #データ整理\n",
        "    transcription = []\n",
        "    for segment in segments:\n",
        "      transcription.append([segment.start, segment.end, segment.text])\n",
        "\n",
        "    #ダウンロードした動画を削除（容量のため）\n",
        "    os.remove(audio_file)\n",
        "\n",
        "    #書き起こしファイルの保存\n",
        "    transcription_file_name = db_dir + str(index) + \".csv\"\n",
        "    df_transcription = pd.DataFrame(data=transcription, columns=['start', 'end', 'text'])\n",
        "    df_transcription.to_csv(transcription_file_name, index = False)\n",
        "\n",
        "    #print(\"transcribed:\", index)\n",
        "\n",
        "    return index"
      ],
      "metadata": {
        "id": "Oq9ukFME8kzH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  transcriber =  Transcriber(model = 4)\n",
        "  transcriber.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2QpbbLON0dR",
        "outputId": "8112b101-532a-4b8c-dd56-d8451c021da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/672 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r_ZhBVXuDMUc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}